{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cindyellow/DynaLR/blob/main/NLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UUSJPdr3Ge_"
      },
      "source": [
        "# Starter code and data\n",
        "\n",
        "First, perform the required imports for your code:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CRwuwhoJ3Knl"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "import pickle\n",
        "import numpy as np\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import pylab\n",
        "from six.moves.urllib.request import urlretrieve\n",
        "import tarfile\n",
        "import sys\n",
        "import itertools\n",
        "from datetime import datetime\n",
        "\n",
        "TINY = 1e-30\n",
        "EPS = 1e-4\n",
        "nax = np.newaxis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNLvRXdy3NDO"
      },
      "source": [
        "If you're using colaboratory, this following script creates a folder - here we used 'CSC413/A1' - in order to download and store the data. If you're not using colaboratory, then set the path to wherever you want the contents to be stored at locally.\n",
        "\n",
        "You can also manually download and unzip the data from [http://www.cs.toronto.edu/~jba/a1_data.tar.gz] and put them in the same folder as where you store this notebook. \n",
        "\n",
        "Feel free to use a different way to access the files *data.pk* , *partially_trained.pk*, and *raw_sentences.txt*. \n",
        "\n",
        "The file *raw_sentences.txt* contains the sentences that we will be using for this assignment.\n",
        "These sentences are fairly simple ones and cover a vocabulary of only 250 words (+ 1 special `[MASK]` token word).\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Gkug8am63SzY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c865b266-deb2-4417-912b-2abd55eb4b19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/CSC413/A1\n"
          ]
        }
      ],
      "source": [
        "######################################################################\n",
        "# Setup working directory\n",
        "######################################################################\n",
        "# Change this to a local path if running locally\n",
        "%mkdir -p /content/CSC413/A1/\n",
        "%cd /content/CSC413/A1\n",
        "\n",
        "######################################################################\n",
        "# Helper functions for loading data\n",
        "######################################################################\n",
        "# adapted from \n",
        "# https://github.com/fchollet/keras/blob/master/keras/datasets/cifar10.py\n",
        "\n",
        "def get_file(fname,\n",
        "             origin,\n",
        "             untar=False,\n",
        "             extract=False,\n",
        "             archive_format='auto',\n",
        "             cache_dir='data'):\n",
        "    datadir = os.path.join(cache_dir)\n",
        "    if not os.path.exists(datadir):\n",
        "        os.makedirs(datadir)\n",
        "\n",
        "    if untar:\n",
        "        untar_fpath = os.path.join(datadir, fname)\n",
        "        fpath = untar_fpath + '.tar.gz'\n",
        "    else:\n",
        "        fpath = os.path.join(datadir, fname)\n",
        "    \n",
        "    print('File path: %s' % fpath)\n",
        "    if not os.path.exists(fpath):\n",
        "        print('Downloading data from', origin)\n",
        "\n",
        "        error_msg = 'URL fetch failure on {}: {} -- {}'\n",
        "        try:\n",
        "            try:\n",
        "                urlretrieve(origin, fpath)\n",
        "            except URLError as e:\n",
        "                raise Exception(error_msg.format(origin, e.errno, e.reason))\n",
        "            except HTTPError as e:\n",
        "                raise Exception(error_msg.format(origin, e.code, e.msg))\n",
        "        except (Exception, KeyboardInterrupt) as e:\n",
        "            if os.path.exists(fpath):\n",
        "                os.remove(fpath)\n",
        "            raise\n",
        "\n",
        "    if untar:\n",
        "        if not os.path.exists(untar_fpath):\n",
        "            print('Extracting file.')\n",
        "            with tarfile.open(fpath) as archive:\n",
        "                archive.extractall(datadir)\n",
        "        return untar_fpath\n",
        "\n",
        "    if extract:\n",
        "        _extract_archive(fpath, datadir, archive_format)\n",
        "\n",
        "    return fpath"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "KUQjRpWqnkzk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e177717b-87cb-41d2-b9c4-6626c243faa5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File path: data/a1_data.tar.gz\n",
            "Downloading data from http://www.cs.toronto.edu/~jba/a1_data.tar.gz\n",
            "Extracting file.\n"
          ]
        }
      ],
      "source": [
        "# Download the dataset and partially pre-trained model\n",
        "get_file(fname='a1_data', \n",
        "                         origin='http://www.cs.toronto.edu/~jba/a1_data.tar.gz', \n",
        "                         untar=True)\n",
        "drive_location = 'data'\n",
        "PARTIALLY_TRAINED_MODEL = drive_location + '/' + 'partially_trained.pk'\n",
        "data_location = drive_location + '/' + 'data.pk'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qna9z_wJ3U5e"
      },
      "source": [
        "We have already extracted the 4-grams from this dataset and divided them into training, validation, and test sets.\n",
        "To inspect this data, run the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "RD1LN16d3a0u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c270910-cf42-44f1-ca5e-2303bbcba82a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MASK]\n",
            "all\n",
            "251\n",
            "['[MASK]', 'all', 'set', 'just', 'show', 'being', 'money', 'over', 'both', 'years', 'four', 'through', 'during', 'go', 'still', 'children', 'before', 'police', 'office', 'million', 'also', 'less', 'had', ',', 'including', 'should', 'to', 'only', 'going', 'under', 'has', 'might', 'do', 'them', 'good', 'around', 'get', 'very', 'big', 'dr.', 'game', 'every', 'know', 'they', 'not', 'world', 'now', 'him', 'school', 'several', 'like', 'did', 'university', 'companies', 'these', 'she', 'team', 'found', 'where', 'right', 'says', 'people', 'house', 'national', 'some', 'back', 'see', 'street', 'are', 'year', 'home', 'best', 'out', 'even', 'what', 'said', 'for', 'federal', 'since', 'its', 'may', 'state', 'does', 'john', 'between', 'new', ';', 'three', 'public', '?', 'be', 'we', 'after', 'business', 'never', 'use', 'here', 'york', 'members', 'percent', 'put', 'group', 'come', 'by', '$', 'on', 'about', 'last', 'her', 'of', 'could', 'days', 'against', 'times', 'women', 'place', 'think', 'first', 'among', 'own', 'family', 'into', 'each', 'one', 'down', 'because', 'long', 'another', 'such', 'old', 'next', 'your', 'market', 'second', 'city', 'little', 'from', 'would', 'few', 'west', 'there', 'political', 'two', 'been', '.', 'their', 'much', 'music', 'too', 'way', 'white', ':', 'was', 'war', 'today', 'more', 'ago', 'life', 'that', 'season', 'company', '-', 'but', 'part', 'court', 'former', 'general', 'with', 'than', 'those', 'he', 'me', 'high', 'made', 'this', 'work', 'up', 'us', 'until', 'will', 'ms.', 'while', 'officials', 'can', 'were', 'country', 'my', 'called', 'and', 'program', 'have', 'then', 'is', 'it', 'an', 'states', 'case', 'say', 'his', 'at', 'want', 'in', 'any', 'as', 'if', 'united', 'end', 'no', ')', 'make', 'government', 'when', 'american', 'same', 'how', 'mr.', 'other', 'take', 'which', 'department', '--', 'you', 'many', 'nt', 'day', 'week', 'play', 'used', \"'s\", 'though', 'our', 'who', 'yesterday', 'director', 'most', 'president', 'law', 'man', 'a', 'night', 'off', 'center', 'i', 'well', 'or', 'without', 'so', 'time', 'five', 'the', 'left']\n",
            "[[ 28  26  90 144]\n",
            " [184  44 249 117]\n",
            " [183  32  76 122]\n",
            " [117 247 201 186]\n",
            " [223 190 249   6]\n",
            " [ 42  74  26  32]\n",
            " [242  32 223  32]\n",
            " [223  32 158 144]\n",
            " [ 74  32 221  32]\n",
            " [ 42 192  91  68]]\n"
          ]
        }
      ],
      "source": [
        "data = pickle.load(open(data_location, 'rb'))\n",
        "print(data['vocab'][0]) # First word in vocab is [MASK] \n",
        "print(data['vocab'][1]) \n",
        "print(len(data['vocab'])) # Number of words in vocab\n",
        "print(data['vocab']) # All the words in vocab\n",
        "print(data['train_inputs'][:10]) # 10 example training instances"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['train_inputs'].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2emiSec7jll",
        "outputId": "107378ec-d67b-4a38-d265-7d5ce8ba9654"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(372500, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['test_inputs'].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXX1QQBh78wo",
        "outputId": "c519cf00-63d7-4646-ab38-c79b1c7a3fc7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(46500, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['valid_inputs'].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFpsKJn87-2c",
        "outputId": "678b5038-e6b1-44e2-f06b-7c9e98cd1fe1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(46500, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXd2Msqs3fPQ"
      },
      "source": [
        "Now `data` is a Python dict which contains the vocabulary, as well as the inputs and targets for all three splits of the data. `data['vocab']` is a list of the 251 words in the dictionary; `data['vocab'][0]` is the word with index 0, and so on. `data['train_inputs']` is a 372,500 x 4 matrix where each row gives the indices of the 4 consecutive context words for one of the 372,500 training cases.\n",
        "The validation and test sets are handled analogously.\n",
        "\n",
        "Even though you only have to modify two specific locations in the code, you may want to read through this code before starting the assignment. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8FZTyrzlCNl"
      },
      "source": [
        "# Neural Language Model: Masked Word Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ua0qkOH1tqHw"
      },
      "source": [
        "In this part, you will learn to implement and train the neural language model from Figure 1. As described in the previous section, during training, we randomly sample one of the $N$ context words to replace with a `[MASK]` token. The goal is for the network to predict the word that was masked, at the corresponding output word position. In practice, this `[MASK]` token is assigned the index 0 in our dictionary. The weights $W^{(2)}$ = `hid_to_output_weights` now has the shape $NV \\times H$, as the output layer has $NV$ neurons, where the first $V$ output units are for predicting the first word, then the next $V$ are for predicting the second word, and so on. \n",
        "        We call this as *concatenating* output units across all word positions, i.e. the $(v + nV)$-th column is for the word $v$ in vocabulary for the $n$-th output word position. \n",
        "        Note here that the softmax is applied in chunks of $V$ as well, to give a valid probability distribution over the $V$ words (For simplicity we also include the `[MASK]` token as one of the possible prediction even though we know the target should not be this token). Only the output word positions that were masked in the input are included in the cross entropy loss calculation:\n",
        "\n",
        "$$C = -\\sum_{i}^{B}\\sum_{n}^{N}\\sum_{v}^{V} m^{(i)}_{n} (t^{(i)}_{v + nV} \\log y^{(i)}_{v + nV})$$\n",
        "\n",
        "Where:\n",
        "*  $y^{(i)}_{v + nV}$ denotes the output probability prediction from the neural network for the $i$-th training example for the word $v$ in the $n$-th output word. Denoting $z$ as the logits output, we define the output probability $y$ as a softmax on $z$ over contiguous chunks of $V$ units (see also Figure 1): \n",
        "\n",
        "$$y^{(i)}_{v + nV} = \\frac{e^{z^{(i)}_{v+nV}}}{\\sum_{l}^{V} e^{z^{(i)}_{l+nV}}}$$\n",
        "* $t^{(i)}_{v + nV}  \\in \\{0,1\\}$ is 1 if for the $i$-th training example, the word $v$ is the $n$-th word in context\n",
        "* $m^{(i)}_{n} \\in \\{0,1\\}$ is a mask that is set to 1 if we are predicting the $n$-th word position for the $i$-th example (because we had masked that word in the input), and 0 otherwise\n",
        "\n",
        "There are three classes defined in this part: `Params`, `Activations`, `Model`.\n",
        "You will make changes to `Model`, but it may help to read through `Params` and `Activations` first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "EfGEjB3QLNXf"
      },
      "outputs": [],
      "source": [
        "class Params(object):\n",
        "    \"\"\"A class representing the trainable parameters of the model. This class has five fields:\n",
        "    \n",
        "           word_embedding_weights, a matrix of size V x D, where V is the number of words in the vocabulary\n",
        "                   and D is the embedding dimension.\n",
        "           embed_to_hid_weights, a matrix of size H x ND, where H is the number of hidden units. The first D\n",
        "                   columns represent connections from the embedding of the first context word, the next D columns\n",
        "                   for the second context word, and so on. There are N context words.\n",
        "           hid_bias, a vector of length H\n",
        "           hid_to_output_weights, a matrix of size NV x H\n",
        "           output_bias, a vector of length NV\"\"\"\n",
        "\n",
        "    def __init__(self, word_embedding_weights, embed_to_hid_weights, hid_to_output_weights,\n",
        "                 hid_bias, output_bias):\n",
        "        self.word_embedding_weights = word_embedding_weights\n",
        "        self.embed_to_hid_weights = embed_to_hid_weights\n",
        "        self.hid_to_output_weights = hid_to_output_weights\n",
        "        self.hid_bias = hid_bias\n",
        "        self.output_bias = output_bias\n",
        "\n",
        "    def copy(self):\n",
        "        return self.__class__(self.word_embedding_weights.copy(), self.embed_to_hid_weights.copy(),\n",
        "                              self.hid_to_output_weights.copy(), self.hid_bias.copy(), self.output_bias.copy())\n",
        "\n",
        "    @classmethod\n",
        "    def zeros(cls, vocab_size, context_len, embedding_dim, num_hid):\n",
        "        \"\"\"A constructor which initializes all weights and biases to 0.\"\"\"\n",
        "        word_embedding_weights = np.zeros((vocab_size, embedding_dim))\n",
        "        embed_to_hid_weights = np.zeros((num_hid, context_len * embedding_dim))\n",
        "        hid_to_output_weights = np.zeros((vocab_size * context_len, num_hid))\n",
        "        hid_bias = np.zeros(num_hid)\n",
        "        output_bias = np.zeros(vocab_size * context_len)\n",
        "        return cls(word_embedding_weights, embed_to_hid_weights, hid_to_output_weights,\n",
        "                   hid_bias, output_bias)\n",
        "\n",
        "    @classmethod\n",
        "    def random_init(cls, init_wt, vocab_size, context_len, embedding_dim, num_hid):\n",
        "        \"\"\"A constructor which initializes weights to small random values and biases to 0.\"\"\"\n",
        "        word_embedding_weights = np.random.normal(0., init_wt, size=(vocab_size, embedding_dim))\n",
        "        embed_to_hid_weights = np.random.normal(0., init_wt, size=(num_hid, context_len * embedding_dim))\n",
        "        hid_to_output_weights = np.random.normal(0., init_wt, size=(vocab_size * context_len, num_hid))\n",
        "        hid_bias = np.zeros(num_hid)\n",
        "        output_bias = np.zeros(vocab_size * context_len)\n",
        "        return cls(word_embedding_weights, embed_to_hid_weights, hid_to_output_weights,\n",
        "                   hid_bias, output_bias)\n",
        "\n",
        "    ###### The functions below are Python's somewhat oddball way of overloading operators, so that\n",
        "    ###### we can do arithmetic on Params instances. You don't need to understand this to do the assignment.\n",
        "\n",
        "    def __mul__(self, a):\n",
        "        return self.__class__(a * self.word_embedding_weights,\n",
        "                              a * self.embed_to_hid_weights,\n",
        "                              a * self.hid_to_output_weights,\n",
        "                              a * self.hid_bias,\n",
        "                              a * self.output_bias)\n",
        "\n",
        "    def __rmul__(self, a):\n",
        "        return self * a\n",
        "\n",
        "    def __add__(self, other):\n",
        "        return self.__class__(self.word_embedding_weights + other.word_embedding_weights,\n",
        "                              self.embed_to_hid_weights + other.embed_to_hid_weights,\n",
        "                              self.hid_to_output_weights + other.hid_to_output_weights,\n",
        "                              self.hid_bias + other.hid_bias,\n",
        "                              self.output_bias + other.output_bias)\n",
        "\n",
        "    def __sub__(self, other):\n",
        "        return self + -1. * other"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "k6XFQUPsLSi7"
      },
      "outputs": [],
      "source": [
        "class Activations(object):\n",
        "    \"\"\"A class representing the activations of the units in the network. This class has three fields:\n",
        "\n",
        "        embedding_layer, a matrix of B x ND matrix (where B is the batch size, D is the embedding dimension,\n",
        "                and N is the number of input context words), representing the activations for the embedding \n",
        "                layer on all the cases in a batch. The first D columns represent the embeddings for the \n",
        "                first context word, and so on.\n",
        "        hidden_layer, a B x H matrix representing the hidden layer activations for a batch\n",
        "        output_layer, a B x V matrix representing the output layer activations for a batch\"\"\"\n",
        "\n",
        "    def __init__(self, embedding_layer, hidden_layer, output_layer):\n",
        "        self.embedding_layer = embedding_layer\n",
        "        self.hidden_layer = hidden_layer\n",
        "        self.output_layer = output_layer\n",
        "\n",
        "def get_batches(inputs, batch_size, shuffle=True):\n",
        "    \"\"\"Divide a dataset (usually the training set) into mini-batches of a given size. This is a\n",
        "    'generator', i.e. something you can use in a for loop. You don't need to understand how it\n",
        "    works to do the assignment.\"\"\"\n",
        "\n",
        "    if inputs.shape[0] % batch_size != 0:\n",
        "        raise RuntimeError('The number of data points must be a multiple of the batch size.')\n",
        "    num_batches = inputs.shape[0] // batch_size\n",
        "\n",
        "    if shuffle:\n",
        "        idxs = np.random.permutation(inputs.shape[0])\n",
        "        inputs = inputs[idxs, :]\n",
        "\n",
        "    for m in range(num_batches):\n",
        "        yield inputs[m * batch_size:(m + 1) * batch_size, :]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuAXaDNll0lf"
      },
      "source": [
        "In this part of the assignment, you implement a method which computes the gradient using backpropagation.\n",
        "To start you out, the *Model* class contains several important methods used in training:\n",
        "\n",
        "\n",
        "*   `compute_activations` computes the activations of all units on a given input batch\n",
        "*   `compute_loss_derivative` computes the gradient with respect to the output logits $\\frac{\\partial C}{\\partial z}$\n",
        "*   `evaluate` computes the average cross-entropy loss for a given set of inputs and targets\n",
        "\n",
        "You will need to complete the implementation of two additional methods to complete the training, and print the outputs of the gradients. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyAoLfRPYZYx"
      },
      "source": [
        "## Implement gradient with respect to parameters\n",
        "`back_propagate` is the function which computes the gradient of the loss with respect to model parameters using backpropagation.\n",
        "It uses the derivatives computed by *compute_loss_derivative*.\n",
        "Some parts are already filled in for you, but you need to compute the matrices of derivatives for `embed_to_hid_weights`, `hid_bias`, `hid_to_output_weights`, and `output_bias`.\n",
        "These matrices have the same sizes as the parameter matrices (see previous section). These matrices have the same sizes as the parameter matrices. Look for the `## YOUR CODE HERE ##` comment for where to complete the code.\n",
        "\n",
        "In order to implement backpropagation efficiently, you need to express the computations in terms of matrix operations, rather than *for* loops.\n",
        "You should first work through the derivatives on pencil and paper.\n",
        "First, apply the chain rule to compute the derivatives with respect to individual units, weights, and biases.\n",
        "Next, take the formulas you've derived, and express them in matrix form.\n",
        "You should be able to express all of the required computations using only matrix multiplication, matrix transpose, and elementwise operations --- no *for* loops!\n",
        "If you want inspiration, read through the code for *Model.compute_activations* and try to understand how the matrix operations correspond to the computations performed by all the units in the network.\n",
        "\n",
        "*Hint: Your implementations should also be similar to* `hid_to_output_weights_grad`,`hid_bias_grad` *in the same function call*\n",
        "\n",
        "*Hint: To prompt a GPT-like model, you may only include functions that are relevent to the implementation in your prompt.*\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "0F4CTBipK9B6"
      },
      "outputs": [],
      "source": [
        "class Model(object):\n",
        "    \"\"\"A class representing the language model itself. This class contains various methods used in training\n",
        "    the model and visualizing the learned representations. It has two fields:\n",
        "\n",
        "        params, a Params instance which contains the model parameters\n",
        "        vocab, a list containing all the words in the dictionary; vocab[0] is the word with index\n",
        "               0, and so on.\"\"\"\n",
        "\n",
        "    def __init__(self, params, vocab):\n",
        "        self.params = params\n",
        "        self.vocab = vocab\n",
        "\n",
        "        self.vocab_size = len(vocab)\n",
        "        self.embedding_dim = self.params.word_embedding_weights.shape[1]\n",
        "        self.embedding_layer_dim = self.params.embed_to_hid_weights.shape[1]\n",
        "        self.context_len = self.embedding_layer_dim // self.embedding_dim\n",
        "        self.num_hid = self.params.embed_to_hid_weights.shape[0]\n",
        "\n",
        "    def copy(self):\n",
        "        return self.__class__(self.params.copy(), self.vocab[:])\n",
        "\n",
        "    @classmethod\n",
        "    def random_init(cls, init_wt, vocab, context_len, embedding_dim, num_hid):\n",
        "        \"\"\"Constructor which randomly initializes the weights to Gaussians with standard deviation init_wt\n",
        "        and initializes the biases to all zeros.\"\"\"\n",
        "        params = Params.random_init(init_wt, len(vocab), context_len, embedding_dim, num_hid)\n",
        "        return Model(params, vocab)\n",
        "\n",
        "    def indicator_matrix(self, targets, mask_zero_index=True):\n",
        "        \"\"\"Construct a matrix where the (v + n*V)th entry of row i is 1 if the n-th target word\n",
        "         for example i is v, and all other entries are 0.\n",
        "\n",
        "         Note: if the n-th target word index is 0, this corresponds to the [MASK] token,\n",
        "               and we set the entry to be 0. \n",
        "        \"\"\"\n",
        "        batch_size, context_len = targets.shape\n",
        "        expanded_targets = np.zeros((batch_size, context_len * len(self.vocab)))\n",
        "        offset = np.repeat((np.arange(context_len) * len(self.vocab))[np.newaxis, :], batch_size, axis=0) # [[0, V, 2V], [0, V, 2V], ...]\n",
        "        targets_offset = targets + offset\n",
        "\n",
        "        for c in range(context_len):\n",
        "          expanded_targets[np.arange(batch_size), targets_offset[:,c]] = 1.\n",
        "          if mask_zero_index:\n",
        "            # Note: Set the targets with index 0, V, 2V to be zero since it corresponds to the [MASK] token\n",
        "            expanded_targets[np.arange(batch_size), offset[:,c]] = 0. \n",
        "        return expanded_targets\n",
        "\n",
        "    def compute_loss_derivative(self, output_activations, expanded_target_batch, target_mask):\n",
        "        \"\"\"Compute the gradient of cross-entropy loss wrt output logits z\n",
        "        \n",
        "            For example:\n",
        "\n",
        "         [y_{0} ....  y_{V-1}] [y_{V}, ..., y_{2*V-1}] [y_{2*V} ... y_{i,3*V-1}] [y_{3*V} ... y_{i,4*V-1}] \n",
        "                  \n",
        "         Where for column v + n*V,\n",
        "\n",
        "            y_{v + n*V} = e^{z_{v + n*V}} / \\sum_{m=0}^{V-1} e^{z_{m + n*V}}, for n=0,...,N-1\n",
        "\n",
        "        This function should return a dC / dz matrix of size [batch_size x (vocab_size * context_len)],\n",
        "        where each row i in dC / dz has columns 0 to V-1 containing the gradient the 1st output \n",
        "        context word from i-th training example, then columns vocab_size to 2*vocab_size - 1 for the 2nd\n",
        "        output context word of the i-th training example, etc.\n",
        "        \n",
        "        C is the loss function summed acrossed all examples as well:\n",
        "\n",
        "            C = -\\sum_{i,j,n} mask_{i,n} (t_{i, j + n*V} log y_{i, j + n*V}), for j=0,...,V, and n=0,...,N\n",
        "\n",
        "        where mask_{i,n} = 1 if the i-th training example has n-th context word as the target, \n",
        "        otherwise mask_{i,n} = 0.\n",
        "        \n",
        "        Args:\n",
        "          output_activations: A [batch_size x (context_len * vocab_size)] matrix, \n",
        "              for the activations of the output layer, i.e. the y_j's.\n",
        "          expanded_target_batch: A [batch_size x (context_len * vocab_size)] matrix, \n",
        "              where expanded_target_batch[i,n*V:(n+1)*V] is the indicator vector for \n",
        "              the n-th context target word position, i.e. the (i, j + n*V) entry is 1 if the \n",
        "              i'th example, the context word at position n is j, and 0 otherwise.\n",
        "          target_mask: A [batch_size x context_len x 1] tensor, where target_mask[i,n] = 1 \n",
        "              if for the i'th example the n-th context word is a target position, otherwise 0\n",
        "        \n",
        "        Outputs:\n",
        "          loss_derivative: A [batch_size x (context_len * vocab_size)] matrix,\n",
        "              where loss_derivative[i,0:vocab_size] contains the gradient\n",
        "              dC / dz_0 for the i-th training example gradient for 1st output \n",
        "              context word, and loss_derivative[i,vocab_size:2*vocab_size] for \n",
        "              the 2nd output context word of the i-th training example, etc.\n",
        "        \"\"\"\n",
        "        # Reshape output_activations and expanded_target_batch and use broadcasting\n",
        "        output_activations_reshape = output_activations.reshape(-1, self.context_len, len(self.vocab))\n",
        "        expanded_target_batch_reshape = expanded_target_batch.reshape(-1, self.context_len, len(self.vocab))\n",
        "        gradient_masked_reshape =  target_mask * (output_activations_reshape - expanded_target_batch_reshape)\n",
        "        gradient_masked = gradient_masked_reshape.reshape(-1, self.context_len * len(self.vocab))\n",
        "        return gradient_masked\n",
        "\n",
        "    def compute_loss(self, output_activations, expanded_target_batch, target_mask):\n",
        "        \"\"\"Compute the total cross entropy loss over a mini-batch.\n",
        "\n",
        "        Args:\n",
        "          output_activations: [batch_size x (context_len * vocab_size)] matrix, \n",
        "                for the activations of the output layer, i.e. the y_j's.\n",
        "          expanded_target_batch: [batch_size (context_len * vocab_size)] matrix, \n",
        "                where expanded_target_batch[i,n*V:(n+1)*V] is the indicator vector for \n",
        "                the n-th context target word position, i.e. the (i, j + n*V) entry is 1 if the \n",
        "                i'th example, the context word at position n is j, and 0 otherwise. matrix obtained\n",
        "          target_mask: A [batch_size x context_len x 1] tensor, where target_mask[i,n,0] = 1 \n",
        "                if for the i'th example the n-th context word is a target position, otherwise 0\n",
        "        \n",
        "        Returns:\n",
        "          loss: a scalar for the  total cross entropy loss over the batch, \n",
        "                defined in Part 3\n",
        "        \"\"\"\n",
        "        ###########################   YOUR CODE HERE  ##############################\n",
        "        mask = target_mask.reshape(target_mask.shape[0], target_mask.shape[1])\n",
        "        mask = np.repeat(mask, self.vocab_size, axis=1)\n",
        "        loss= -np.sum(mask*expanded_target_batch*np.log(output_activations))\n",
        "        ############################################################################\n",
        "        return loss\n",
        "\n",
        "    def compute_activations(self, inputs):\n",
        "        \"\"\"Compute the activations on a batch given the inputs. Returns an Activations instance.\n",
        "        You should try to read and understand this function, since this will give you clues for\n",
        "        how to implement back_propagate.\"\"\"\n",
        "\n",
        "        batch_size = inputs.shape[0]\n",
        "        if inputs.shape[1] != self.context_len:\n",
        "            raise RuntimeError('Dimension of the input vectors should be {}, but is instead {}'.format(\n",
        "                self.context_len, inputs.shape[1]))\n",
        "\n",
        "        # Embedding layer\n",
        "        # Look up the input word indices in the word_embedding_weights matrix\n",
        "        embedding_layer_state = self.params.word_embedding_weights[inputs.reshape([-1]), :].reshape([batch_size, self.embedding_layer_dim])\n",
        "\n",
        "        # Hidden layer\n",
        "        inputs_to_hid = np.dot(embedding_layer_state, self.params.embed_to_hid_weights.T) + \\\n",
        "                        self.params.hid_bias\n",
        "        # Apply logistic activation function\n",
        "        hidden_layer_state = 1. / (1. + np.exp(-inputs_to_hid))\n",
        "\n",
        "        # Output layer\n",
        "        inputs_to_softmax = np.dot(hidden_layer_state, self.params.hid_to_output_weights.T) + \\\n",
        "                            self.params.output_bias\n",
        "\n",
        "        # Subtract maximum.\n",
        "        # Remember that adding or subtracting the same constant from each input to a\n",
        "        # softmax unit does not affect the outputs. So subtract the maximum to\n",
        "        # make all inputs <= 0. This prevents overflows when computing their exponents.\n",
        "        inputs_to_softmax -= inputs_to_softmax.max(1).reshape((-1, 1))\n",
        "\n",
        "        # Take softmax along each V chunks in the output layer\n",
        "        output_layer_state = np.exp(inputs_to_softmax)\n",
        "        output_layer_state_shape = output_layer_state.shape\n",
        "        output_layer_state = output_layer_state.reshape((-1, self.context_len, len(self.vocab)))\n",
        "        output_layer_state /= output_layer_state.sum(axis=-1, keepdims=True) # Softmax along vocab of each target word\n",
        "        output_layer_state = output_layer_state.reshape(output_layer_state_shape) # Flatten back to 2D matrix\n",
        "\n",
        "        return Activations(embedding_layer_state, hidden_layer_state, output_layer_state)\n",
        "\n",
        "    def back_propagate(self, input_batch, activations, loss_derivative):\n",
        "        \"\"\"Compute the gradient of the loss function with respect to the trainable parameters\n",
        "        of the model.\n",
        "        \n",
        "        Part of this function is already completed, but you need to fill in the derivative\n",
        "        computations for hid_to_output_weights_grad, output_bias_grad, embed_to_hid_weights_grad,\n",
        "        and hid_bias_grad. See the documentation for the Params class for a description of what\n",
        "        these matrices represent.\n",
        "\n",
        "        Args: \n",
        "          input_batch: A [batch_size x context_length] matrix containing the \n",
        "              indices of the context words\n",
        "          activations: an Activations object representing the output of \n",
        "              Model.compute_activations\n",
        "          loss_derivative:  A [batch_size x (context_len * vocab_size)] matrix,\n",
        "              where loss_derivative[i,0:vocab_size] contains the gradient\n",
        "              dC / dz_0 for the i-th training example gradient for 1st output \n",
        "              context word, and loss_derivative[i,vocab_size:2*vocab_size] for \n",
        "              the 2nd output context word of the i-th training example, etc.\n",
        "              Obtained from calling compute_loss_derivative()\n",
        "          \n",
        "        Returns:\n",
        "          Params object containing the gradient for word_embedding_weights_grad, \n",
        "              embed_to_hid_weights_grad, hid_to_output_weights_grad,\n",
        "              hid_bias_grad, output_bias_grad  \n",
        "        \"\"\"\n",
        "\n",
        "        # The matrix with values dC / dz_j, where dz_j is the input to the jth hidden unit,\n",
        "        # i.e. h_j = 1 / (1 + e^{-z_j})\n",
        "        hid_deriv = np.dot(loss_derivative, self.params.hid_to_output_weights) \\\n",
        "                    * activations.hidden_layer * (1. - activations.hidden_layer)\n",
        "\n",
        "        \n",
        "        hid_to_output_weights_grad = np.dot(loss_derivative.T, activations.hidden_layer)\n",
        "        \n",
        "        ###########################   YOUR CODE HERE  ##############################\n",
        "        output_bias_grad = loss_derivative.sum(0)\n",
        "        embed_to_hid_weights_grad = np.dot(hid_deriv.T, activations.embedding_layer)\n",
        "        ############################################################################\n",
        "        \n",
        "        hid_bias_grad = hid_deriv.sum(0)\n",
        "\n",
        "        # The matrix of derivatives for the embedding layer\n",
        "        embed_deriv = np.dot(hid_deriv, self.params.embed_to_hid_weights)\n",
        "\n",
        "        # Word Embedding Weights gradient\n",
        "        word_embedding_weights_grad = np.dot(self.indicator_matrix(input_batch.reshape([-1,1]), mask_zero_index=False).T, \n",
        "                                                 embed_deriv.reshape([-1, self.embedding_dim]))\n",
        "\n",
        "        return Params(word_embedding_weights_grad, embed_to_hid_weights_grad, hid_to_output_weights_grad,\n",
        "                      hid_bias_grad, output_bias_grad)\n",
        "\n",
        "    def sample_input_mask(self, batch_size):\n",
        "        \"\"\"Samples a binary mask for the inputs of size batch_size x context_len\n",
        "        For each row, at most one element will be 1.\n",
        "        \"\"\"\n",
        "        mask_idx = np.random.randint(self.context_len, size=(batch_size,))\n",
        "        mask = np.zeros((batch_size, self.context_len), dtype=np.int)# Convert to one hot B x N, B batch size, N context len\n",
        "        mask[np.arange(batch_size), mask_idx] = 1\n",
        "        return mask\n",
        "    \n",
        "    def evaluate(self, inputs, batch_size=100):\n",
        "        \"\"\"Compute the average cross-entropy over a dataset.\n",
        "\n",
        "            inputs: matrix of shape D x N\"\"\"\n",
        "\n",
        "        ndata = inputs.shape[0]\n",
        "\n",
        "        total = 0.\n",
        "        for input_batch in get_batches(inputs, batch_size):\n",
        "            mask = self.sample_input_mask(batch_size)\n",
        "            input_batch_masked = input_batch * (1 - mask)\n",
        "            activations = self.compute_activations(input_batch_masked)\n",
        "            expanded_target_batch = self.indicator_matrix(input_batch)\n",
        "            target_mask = np.expand_dims(mask, axis=2)\n",
        "            cross_entropy = self.compute_loss(activations.output_layer, expanded_target_batch, target_mask)\n",
        "            total += cross_entropy\n",
        "\n",
        "        return total / float(ndata)\n",
        "\n",
        "    def display_nearest_words(self, word, k=10):\n",
        "        \"\"\"List the k words nearest to a given word, along with their distances.\"\"\"\n",
        "\n",
        "        if word not in self.vocab:\n",
        "            print('Word \"{}\" not in vocabulary.'.format(word))\n",
        "            return\n",
        "\n",
        "        # Compute distance to every other word.\n",
        "        idx = self.vocab.index(word)\n",
        "        word_rep = self.params.word_embedding_weights[idx, :]\n",
        "        diff = self.params.word_embedding_weights - word_rep.reshape((1, -1))\n",
        "        distance = np.sqrt(np.sum(diff ** 2, axis=1))\n",
        "\n",
        "        # Sort by distance.\n",
        "        order = np.argsort(distance)\n",
        "        order = order[1:1 + k]  # The nearest word is the query word itself, skip that.\n",
        "        for i in order:\n",
        "            print('{}: {}'.format(self.vocab[i], distance[i]))\n",
        "\n",
        "    def word_distance(self, word1, word2):\n",
        "        \"\"\"Compute the distance between the vector representations of two words.\"\"\"\n",
        "\n",
        "        if word1 not in self.vocab:\n",
        "            raise RuntimeError('Word \"{}\" not in vocabulary.'.format(word1))\n",
        "        if word2 not in self.vocab:\n",
        "            raise RuntimeError('Word \"{}\" not in vocabulary.'.format(word2))\n",
        "\n",
        "        idx1, idx2 = self.vocab.index(word1), self.vocab.index(word2)\n",
        "        word_rep1 = self.params.word_embedding_weights[idx1, :]\n",
        "        word_rep2 = self.params.word_embedding_weights[idx2, :]\n",
        "        diff = word_rep1 - word_rep2\n",
        "        return np.sqrt(np.sum(diff ** 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtC-br-N5xGT"
      },
      "source": [
        "## Run model training with different optimizers\n",
        "\n",
        "Once you've implemented the gradient computation, you'll need to train the model.\n",
        "The function *train* implements the main training procedure.\n",
        "It takes two arguments:\n",
        "\n",
        "\n",
        "*   `embedding_dim`: The number of dimensions in the distributed representation.\n",
        "*   `num_hid`: The number of hidden units\n",
        "\n",
        "\n",
        "As the model trains, the script prints out some numbers that tell you how well the training is going.\n",
        "It shows:\n",
        "\n",
        "\n",
        "*   The cross entropy on the last 100 mini-batches of the training set. This is shown after every 100 mini-batches.\n",
        "*   The cross entropy on the entire validation set every 1000 mini-batches of training.\n",
        "\n",
        "At the end of training, this function shows the cross entropies on the training, validation and test sets.\n",
        "It will return a *Model* instance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "akBYJQOdLfaF"
      },
      "outputs": [],
      "source": [
        "#### THIS IS THE CODE CHUNK\n",
        "\n",
        "_train_inputs = None\n",
        "_train_targets = None\n",
        "_vocab = None\n",
        "\n",
        "DEFAULT_TRAINING_CONFIG = {'batch_size': 3725,  # the size of a mini-batch\n",
        "                           \n",
        "                           #'learning_rate': 0.1,  # the learning rate\n",
        "                           'fixed_learning_rates': [0.01, 0.1, 0.5],\n",
        "                          #  'fixed_learning_rates': [0.1],\n",
        "                           'momentum': 0.9,  # the decay parameter for the momentum vector\n",
        "                           'epochs': 50,  # the maximum number of epochs to run\n",
        "                           'init_wt': 0.01,  # the standard deviation of the initial random weights\n",
        "                           'context_len': 4,  # the number of context words used\n",
        "                           'show_training_CE_after': 10,  # measure training error after this many mini-batches\n",
        "                           'show_validation_CE_after': 10,  # measure validation error after this many mini-batches\n",
        "                           }\n",
        "\n",
        "\n",
        "def find_occurrences(word1, word2, word3):\n",
        "    \"\"\"Lists all the words that followed a given tri-gram in the training set and the number of\n",
        "    times each one followed it.\"\"\"\n",
        "\n",
        "    # cache the data so we don't keep reloading\n",
        "    global _train_inputs, _train_targets, _vocab\n",
        "    if _train_inputs is None:\n",
        "        data_obj = pickle.load(open(data_location, 'rb'))\n",
        "        _vocab = data_obj['vocab']\n",
        "        _train_inputs, _train_targets = data_obj['train_inputs'], data_obj['train_targets']\n",
        "\n",
        "    if word1 not in _vocab:\n",
        "        raise RuntimeError('Word \"{}\" not in vocabulary.'.format(word1))\n",
        "    if word2 not in _vocab:\n",
        "        raise RuntimeError('Word \"{}\" not in vocabulary.'.format(word2))\n",
        "    if word3 not in _vocab:\n",
        "        raise RuntimeError('Word \"{}\" not in vocabulary.'.format(word3))\n",
        "\n",
        "    idx1, idx2, idx3 = _vocab.index(word1), _vocab.index(word2), _vocab.index(word3)\n",
        "    idxs = np.array([idx1, idx2, idx3])\n",
        "\n",
        "    matches = np.all(_train_inputs == idxs.reshape((1, -1)), 1)\n",
        "\n",
        "    if np.any(matches):\n",
        "        counts = collections.defaultdict(int)\n",
        "        for m in np.where(matches)[0]:\n",
        "            counts[_vocab[_train_targets[m]]] += 1\n",
        "\n",
        "        word_counts = sorted(list(counts.items()), key=lambda t: t[1], reverse=True)\n",
        "        print('The tri-gram \"{} {} {}\" was followed by the following words in the training set:'.format(\n",
        "            word1, word2, word3))\n",
        "        for word, count in word_counts:\n",
        "            if count > 1:\n",
        "                print('    {} ({} times)'.format(word, count))\n",
        "            else:\n",
        "                print('    {} (1 time)'.format(word))\n",
        "    else:\n",
        "        print('The tri-gram \"{} {} {}\" did not occur in the training set.'.format(word1, word2, word3))\n",
        "\n",
        "\n",
        "def train(embedding_dim, num_hid, config=DEFAULT_TRAINING_CONFIG):\n",
        "    \"\"\"This is the main training routine for the language model. It takes two parameters:\n",
        "\n",
        "        embedding_dim, the dimension of the embedding space\n",
        "        num_hid, the number of hidden units.\"\"\"\n",
        "\n",
        "    ########################\n",
        "    # For reproducibility\n",
        "    np.random.seed(123)\n",
        "    ########################\n",
        "\n",
        "    # Load the data\n",
        "    data_obj = pickle.load(open(data_location, 'rb'))\n",
        "    vocab = data_obj['vocab']\n",
        "    train_inputs = data_obj['train_inputs']\n",
        "    valid_inputs = data_obj['valid_inputs']\n",
        "    test_inputs = data_obj['test_inputs']\n",
        "\n",
        "    # Randomly initialize the trainable parameters\n",
        "\n",
        "    model = Model.random_init(config['init_wt'], vocab, config['context_len'], embedding_dim, num_hid)\n",
        "\n",
        "    # Variables used for early stopping\n",
        "    best_valid_CE = np.infty\n",
        "    end_training = False\n",
        "\n",
        "    # Initialize the momentum vector to all zeros\n",
        "    momentum_delta = Params.zeros(len(vocab), config['context_len'], embedding_dim, num_hid)\n",
        "\n",
        "    start_time = datetime.now()\n",
        "\n",
        "    this_chunk_CE = 0.\n",
        "    batch_count = 0\n",
        "    for epoch in range(1, config['epochs'] + 1):  \n",
        "        if end_training:\n",
        "            break\n",
        "\n",
        "        print()\n",
        "        print('Epoch', epoch)\n",
        "\n",
        "        for m, (input_batch) in enumerate(get_batches(train_inputs, config['batch_size'])):\n",
        "            batch_count += 1\n",
        "            print('Batch {}'.format(batch_count))\n",
        "\n",
        "            # For each example (row in input_batch), select one word to mask out\n",
        "            mask = model.sample_input_mask(config['batch_size'])\n",
        "            input_batch_masked = input_batch * (1 - mask) # We only zero out one word per row\n",
        "\n",
        "            # Forward propagate\n",
        "            activations = model.compute_activations(input_batch_masked)\n",
        "\n",
        "            # Compute loss derivative            \n",
        "            expanded_target_batch = model.indicator_matrix(input_batch)\n",
        "            loss_derivative = model.compute_loss_derivative(activations.output_layer, expanded_target_batch, mask[:,:, np.newaxis])\n",
        "            loss_derivative /= config['batch_size']\n",
        "\n",
        "            # Measure loss function\n",
        "            cross_entropy = model.compute_loss(activations.output_layer, expanded_target_batch, np.expand_dims(mask, axis=2)) / config['batch_size']\n",
        "            this_chunk_CE += cross_entropy\n",
        "            if batch_count % config['show_training_CE_after'] == 0:\n",
        "                print('Batch {} Train CE {:1.3f}'.format(\n",
        "                    batch_count, this_chunk_CE / config['show_training_CE_after']))\n",
        "                this_chunk_CE = 0.\n",
        "\n",
        "            # Backpropagate\n",
        "            loss_gradient = model.back_propagate(input_batch, activations, loss_derivative)\n",
        "\n",
        "            # Update the momentum vector and model parameters\n",
        "            momentum_delta = config['momentum'] * momentum_delta + loss_gradient\n",
        "            fixed_delta = loss_gradient\n",
        "\n",
        "            #####################################\n",
        "            best_loss = np.inf\n",
        "            best_params_i = -1\n",
        "            fixed_learning_rates = config['fixed_learning_rates']\n",
        "\n",
        "            # Without momentum\n",
        "            for i in range(len(fixed_learning_rates)):\n",
        "                learning_rate = fixed_learning_rates[i]\n",
        "                model.params -= learning_rate * fixed_delta\n",
        "                curr_loss = model.evaluate(valid_inputs)\n",
        "                if curr_loss < best_loss:\n",
        "                    best_loss = curr_loss\n",
        "                    best_params_i = i\n",
        "                model.params += learning_rate * fixed_delta\n",
        "\n",
        "            # With momentum\n",
        "            for i in range(len(fixed_learning_rates)):\n",
        "                learning_rate = fixed_learning_rates[i]\n",
        "                model.params -= learning_rate * momentum_delta\n",
        "                curr_loss = model.evaluate(valid_inputs)\n",
        "                if curr_loss < best_loss:\n",
        "                    best_loss = curr_loss\n",
        "                    best_params_i = i + len(fixed_learning_rates)\n",
        "                model.params += learning_rate * momentum_delta\n",
        "\n",
        "            # Update\n",
        "            if best_params_i >= len(fixed_learning_rates):\n",
        "              learning_rate = fixed_learning_rates[best_params_i - len(fixed_learning_rates)]\n",
        "              model.params -= learning_rate * momentum_delta\n",
        "            else:\n",
        "              learning_rate = fixed_learning_rates[best_params_i]\n",
        "              model.params -= learning_rate * fixed_delta\n",
        "            #####################################\n",
        "\n",
        "\n",
        "            # Validate\n",
        "            if batch_count % config['show_validation_CE_after'] == 0:\n",
        "                print('Running validation...')\n",
        "                cross_entropy = model.evaluate(valid_inputs)\n",
        "                print('Validation cross-entropy: {:1.3f}'.format(cross_entropy))\n",
        "\n",
        "                if cross_entropy > best_valid_CE:\n",
        "                    print('Validation error increasing!  Training stopped.')\n",
        "                    end_training = True\n",
        "                    break\n",
        "\n",
        "                best_valid_CE = cross_entropy\n",
        "\n",
        "    print()\n",
        "    end_time = datetime.now()\n",
        "    print('Duration: {}'.format(end_time - start_time))\n",
        "    train_CE = model.evaluate(train_inputs)\n",
        "    print('Final training cross-entropy: {:1.3f}'.format(train_CE))\n",
        "    valid_CE = model.evaluate(valid_inputs)\n",
        "    print('Final validation cross-entropy: {:1.3f}'.format(valid_CE))\n",
        "    test_CE = model.evaluate(test_inputs)\n",
        "    print('Final test cross-entropy: {:1.3f}'.format(test_CE))\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZX-g3K-F55h7"
      },
      "source": [
        "Run the training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "BwlRG7j8LmIM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f2b958f-69a4-492f-a5ad-47a6e6915ae3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1\n",
            "Batch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-896f1c86448c>:215: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  mask = np.zeros((batch_size, self.context_len), dtype=np.int)# Convert to one hot B x N, B batch size, N context len\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 2\n",
            "Batch 3\n",
            "Batch 4\n",
            "Batch 5\n",
            "Batch 6\n",
            "Batch 7\n",
            "Batch 8\n",
            "Batch 9\n",
            "Batch 10\n",
            "Batch 10 Train CE 5.067\n",
            "Running validation...\n",
            "Validation cross-entropy: 4.725\n",
            "Batch 11\n",
            "Batch 12\n",
            "Batch 13\n",
            "Batch 14\n",
            "Batch 15\n",
            "Batch 16\n",
            "Batch 17\n",
            "Batch 18\n",
            "Batch 19\n",
            "Batch 20\n",
            "Batch 20 Train CE 4.677\n",
            "Running validation...\n",
            "Validation cross-entropy: 4.630\n",
            "Batch 21\n",
            "Batch 22\n",
            "Batch 23\n",
            "Batch 24\n",
            "Batch 25\n",
            "Batch 26\n",
            "Batch 27\n",
            "Batch 28\n",
            "Batch 29\n",
            "Batch 30\n",
            "Batch 30 Train CE 4.609\n",
            "Running validation...\n",
            "Validation cross-entropy: 4.596\n",
            "Batch 31\n",
            "Batch 32\n",
            "Batch 33\n",
            "Batch 34\n",
            "Batch 35\n",
            "Batch 36\n",
            "Batch 37\n",
            "Batch 38\n",
            "Batch 39\n",
            "Batch 40\n",
            "Batch 40 Train CE 4.599\n",
            "Running validation...\n",
            "Validation cross-entropy: 4.588\n",
            "Batch 41\n",
            "Batch 42\n",
            "Batch 43\n",
            "Batch 44\n",
            "Batch 45\n",
            "Batch 46\n",
            "Batch 47\n",
            "Batch 48\n",
            "Batch 49\n",
            "Batch 50\n",
            "Batch 50 Train CE 4.584\n",
            "Running validation...\n",
            "Validation cross-entropy: 4.586\n",
            "Batch 51\n",
            "Batch 52\n",
            "Batch 53\n",
            "Batch 54\n",
            "Batch 55\n",
            "Batch 56\n",
            "Batch 57\n",
            "Batch 58\n",
            "Batch 59\n",
            "Batch 60\n",
            "Batch 60 Train CE 4.572\n",
            "Running validation...\n",
            "Validation cross-entropy: 4.576\n",
            "Batch 61\n",
            "Batch 62\n",
            "Batch 63\n",
            "Batch 64\n",
            "Batch 65\n",
            "Batch 66\n",
            "Batch 67\n",
            "Batch 68\n",
            "Batch 69\n",
            "Batch 70\n",
            "Batch 70 Train CE 4.577\n",
            "Running validation...\n",
            "Validation cross-entropy: 4.574\n",
            "Batch 71\n",
            "Batch 72\n",
            "Batch 73\n",
            "Batch 74\n",
            "Batch 75\n",
            "Batch 76\n",
            "Batch 77\n",
            "Batch 78\n",
            "Batch 79\n",
            "Batch 80\n",
            "Batch 80 Train CE 4.576\n",
            "Running validation...\n",
            "Validation cross-entropy: 4.568\n",
            "Batch 81\n",
            "Batch 82\n",
            "Batch 83\n",
            "Batch 84\n",
            "Batch 85\n",
            "Batch 86\n",
            "Batch 87\n",
            "Batch 88\n",
            "Batch 89\n",
            "Batch 90\n",
            "Batch 90 Train CE 4.573\n",
            "Running validation...\n",
            "Validation cross-entropy: 4.576\n",
            "Validation error increasing!  Training stopped.\n",
            "\n",
            "Duration: 0:34:14.441700\n",
            "Final training cross-entropy: 4.570\n",
            "Final validation cross-entropy: 4.564\n",
            "Final test cross-entropy: 4.585\n"
          ]
        }
      ],
      "source": [
        "# Varied learning rate\n",
        "embedding_dim = 16\n",
        "num_hid = 128\n",
        "trained_model = train(embedding_dim, num_hid)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SGD without momentum\n",
        "embedding_dim = 16\n",
        "num_hid = 128\n",
        "trained_model = train(embedding_dim, num_hid)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_8AvqmcqCei",
        "outputId": "e457436b-c240-4256-f7bc-43f784f5f199"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1\n",
            "Batch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-896f1c86448c>:215: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  mask = np.zeros((batch_size, self.context_len), dtype=np.int)# Convert to one hot B x N, B batch size, N context len\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 2\n",
            "Batch 3\n",
            "Batch 4\n",
            "Batch 5\n",
            "Batch 6\n",
            "Batch 7\n",
            "Batch 8\n",
            "Batch 9\n",
            "Batch 10\n",
            "Batch 10 Train CE 5.458\n",
            "Running validation...\n",
            "Validation cross-entropy: 5.378\n",
            "Batch 11\n",
            "Batch 12\n",
            "Batch 13\n",
            "Batch 14\n",
            "Batch 15\n",
            "Batch 16\n",
            "Batch 17\n",
            "Batch 18\n",
            "Batch 19\n",
            "Batch 20\n",
            "Batch 20 Train CE 5.317\n",
            "Running validation...\n",
            "Validation cross-entropy: 5.248\n",
            "Batch 21\n",
            "Batch 22\n",
            "Batch 23\n",
            "Batch 24\n",
            "Batch 25\n",
            "Batch 26\n",
            "Batch 27\n",
            "Batch 28\n",
            "Batch 29\n",
            "Batch 30\n",
            "Batch 30 Train CE 5.203\n",
            "Running validation...\n",
            "Validation cross-entropy: 5.153\n",
            "Batch 31\n",
            "Batch 32\n",
            "Batch 33\n",
            "Batch 34\n",
            "Batch 35\n",
            "Batch 36\n",
            "Batch 37\n",
            "Batch 38\n",
            "Batch 39\n",
            "Batch 40\n",
            "Batch 40 Train CE 5.121\n",
            "Running validation...\n",
            "Validation cross-entropy: 5.090\n",
            "Batch 41\n",
            "Batch 42\n",
            "Batch 43\n",
            "Batch 44\n",
            "Batch 45\n",
            "Batch 46\n",
            "Batch 47\n",
            "Batch 48\n",
            "Batch 49\n",
            "Batch 50\n",
            "Batch 50 Train CE 5.057\n",
            "Running validation...\n",
            "Validation cross-entropy: 5.027\n",
            "Batch 51\n",
            "Batch 52\n",
            "Batch 53\n",
            "Batch 54\n",
            "Batch 55\n",
            "Batch 56\n",
            "Batch 57\n",
            "Batch 58\n",
            "Batch 59\n",
            "Batch 60\n",
            "Batch 60 Train CE 4.997\n",
            "Running validation...\n",
            "Validation cross-entropy: 4.972\n",
            "Batch 61\n",
            "Batch 62\n",
            "Batch 63\n",
            "Batch 64\n",
            "Batch 65\n",
            "Batch 66\n",
            "Batch 67\n",
            "Batch 68\n",
            "Batch 69\n",
            "Batch 70\n",
            "Batch 70 Train CE 4.954\n",
            "Running validation...\n",
            "Validation cross-entropy: 4.931\n",
            "Batch 71\n",
            "Batch 72\n",
            "Batch 73\n",
            "Batch 74\n",
            "Batch 75\n",
            "Batch 76\n",
            "Batch 77\n",
            "Batch 78\n",
            "Batch 79\n",
            "Batch 80\n",
            "Batch 80 Train CE 4.912\n",
            "Running validation...\n",
            "Validation cross-entropy: 4.898\n",
            "Batch 81\n",
            "Batch 82\n",
            "Batch 83\n",
            "Batch 84\n",
            "Batch 85\n",
            "Batch 86\n",
            "Batch 87\n",
            "Batch 88\n",
            "Batch 89\n",
            "Batch 90\n",
            "Batch 90 Train CE 4.878\n",
            "Running validation...\n",
            "Validation cross-entropy: 4.867\n",
            "Batch 91\n",
            "Batch 92\n",
            "Batch 93\n",
            "Batch 94\n",
            "Batch 95\n",
            "Batch 96\n",
            "Batch 97\n",
            "Batch 98\n",
            "Batch 99\n",
            "Batch 100\n",
            "Batch 100 Train CE 4.842\n",
            "Running validation...\n",
            "Validation cross-entropy: 4.843\n",
            "\n",
            "Epoch 2\n",
            "Batch 101\n",
            "Batch 102\n",
            "Batch 103\n",
            "Batch 104\n",
            "Batch 105\n",
            "Batch 106\n",
            "Batch 107\n",
            "Batch 108\n",
            "Batch 109\n",
            "Batch 110\n",
            "Batch 110 Train CE 4.823\n",
            "Running validation...\n",
            "Validation cross-entropy: 4.815\n",
            "Batch 111\n",
            "Batch 112\n",
            "Batch 113\n",
            "Batch 114\n",
            "Batch 115\n",
            "Batch 116\n",
            "Batch 117\n",
            "Batch 118\n",
            "Batch 119\n",
            "Batch 120\n",
            "Batch 120 Train CE 4.801\n",
            "Running validation...\n",
            "Validation cross-entropy: 4.798\n",
            "Batch 121\n",
            "Batch 122\n",
            "Batch 123\n",
            "Batch 124\n",
            "Batch 125\n",
            "Batch 126\n",
            "Batch 127\n",
            "Batch 128\n",
            "Batch 129\n",
            "Batch 130\n",
            "Batch 130 Train CE 4.780\n",
            "Running validation...\n",
            "Validation cross-entropy: 4.775\n",
            "Batch 131\n",
            "Batch 132\n",
            "Batch 133\n",
            "Batch 134\n",
            "Batch 135\n",
            "Batch 136\n",
            "Batch 137\n",
            "Batch 138\n",
            "Batch 139\n",
            "Batch 140\n",
            "Batch 140 Train CE 4.766\n",
            "Running validation...\n",
            "Validation cross-entropy: 4.769\n",
            "Batch 141\n",
            "Batch 142\n",
            "Batch 143\n",
            "Batch 144\n",
            "Batch 145\n",
            "Batch 146\n",
            "Batch 147\n",
            "Batch 148\n",
            "Batch 149\n",
            "Batch 150\n",
            "Batch 150 Train CE 4.756\n",
            "Running validation...\n",
            "Validation cross-entropy: 4.741\n",
            "Batch 151\n",
            "Batch 152\n",
            "Batch 153\n",
            "Batch 154\n",
            "Batch 155\n",
            "Batch 156\n",
            "Batch 157\n",
            "Batch 158\n",
            "Batch 159\n",
            "Batch 160\n",
            "Batch 160 Train CE 4.730\n",
            "Running validation...\n",
            "Validation cross-entropy: 4.738\n",
            "Batch 161\n",
            "Batch 162\n",
            "Batch 163\n",
            "Batch 164\n",
            "Batch 165\n",
            "Batch 166\n",
            "Batch 167\n",
            "Batch 168\n",
            "Batch 169\n",
            "Batch 170\n",
            "Batch 170 Train CE 4.730\n",
            "Running validation...\n",
            "Validation cross-entropy: 4.729\n",
            "Batch 171\n",
            "Batch 172\n",
            "Batch 173\n",
            "Batch 174\n",
            "Batch 175\n",
            "Batch 176\n",
            "Batch 177\n",
            "Batch 178\n",
            "Batch 179\n",
            "Batch 180\n",
            "Batch 180 Train CE 4.718\n",
            "Running validation...\n",
            "Validation cross-entropy: 4.715\n",
            "Batch 181\n",
            "Batch 182\n",
            "Batch 183\n",
            "Batch 184\n",
            "Batch 185\n",
            "Batch 186\n",
            "Batch 187\n",
            "Batch 188\n",
            "Batch 189\n",
            "Batch 190\n",
            "Batch 190 Train CE 4.713\n",
            "Running validation...\n",
            "Validation cross-entropy: 4.706\n",
            "Batch 191\n",
            "Batch 192\n",
            "Batch 193\n",
            "Batch 194\n",
            "Batch 195\n",
            "Batch 196\n",
            "Batch 197\n",
            "Batch 198\n",
            "Batch 199\n",
            "Batch 200\n",
            "Batch 200 Train CE 4.707\n",
            "Running validation...\n",
            "Validation cross-entropy: 4.702\n",
            "\n",
            "Epoch 3\n",
            "Batch 201\n",
            "Batch 202\n",
            "Batch 203\n",
            "Batch 204\n",
            "Batch 205\n",
            "Batch 206\n",
            "Batch 207\n",
            "Batch 208\n",
            "Batch 209\n",
            "Batch 210\n",
            "Batch 210 Train CE 4.692\n",
            "Running validation...\n",
            "Validation cross-entropy: 4.697\n",
            "Batch 211\n",
            "Batch 212\n",
            "Batch 213\n",
            "Batch 214\n",
            "Batch 215\n",
            "Batch 216\n",
            "Batch 217\n",
            "Batch 218\n",
            "Batch 219\n",
            "Batch 220\n",
            "Batch 220 Train CE 4.697\n",
            "Running validation...\n",
            "Validation cross-entropy: 4.683\n",
            "Batch 221\n",
            "Batch 222\n",
            "Batch 223\n",
            "Batch 224\n",
            "Batch 225\n",
            "Batch 226\n",
            "Batch 227\n",
            "Batch 228\n",
            "Batch 229\n",
            "Batch 230\n",
            "Batch 230 Train CE 4.689\n",
            "Running validation...\n",
            "Validation cross-entropy: 4.668\n",
            "Batch 231\n",
            "Batch 232\n",
            "Batch 233\n",
            "Batch 234\n",
            "Batch 235\n",
            "Batch 236\n",
            "Batch 237\n",
            "Batch 238\n",
            "Batch 239\n",
            "Batch 240\n",
            "Batch 240 Train CE 4.674\n",
            "Running validation...\n",
            "Validation cross-entropy: 4.669\n",
            "Validation error increasing!  Training stopped.\n",
            "\n",
            "Duration: 0:18:32.858581\n",
            "Final training cross-entropy: 4.673\n",
            "Final validation cross-entropy: 4.678\n",
            "Final test cross-entropy: 4.683\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SGD with momentum\n",
        "embedding_dim = 16\n",
        "num_hid = 128\n",
        "trained_model = train(embedding_dim, num_hid)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RjTf9KeVHbv3",
        "outputId": "6223f598-c7eb-4a55-8ef4-3eb889842154"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1\n",
            "Batch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-896f1c86448c>:215: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  mask = np.zeros((batch_size, self.context_len), dtype=np.int)# Convert to one hot B x N, B batch size, N context len\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 2\n",
            "Batch 3\n",
            "Batch 4\n",
            "Batch 5\n",
            "Batch 6\n",
            "Batch 7\n",
            "Batch 8\n",
            "Batch 9\n",
            "Batch 10\n",
            "Batch 10 Train CE 5.345\n",
            "Running validation...\n",
            "Validation cross-entropy: 5.076\n",
            "Batch 11\n",
            "Batch 12\n",
            "Batch 13\n",
            "Batch 14\n",
            "Batch 15\n",
            "Batch 16\n",
            "Batch 17\n",
            "Batch 18\n",
            "Batch 19\n",
            "Batch 20\n",
            "Batch 20 Train CE 4.908\n",
            "Running validation...\n",
            "Validation cross-entropy: 4.773\n",
            "Batch 21\n",
            "Batch 22\n",
            "Batch 23\n",
            "Batch 24\n",
            "Batch 25\n",
            "Batch 26\n",
            "Batch 27\n",
            "Batch 28\n",
            "Batch 29\n",
            "Batch 30\n",
            "Batch 30 Train CE 4.719\n",
            "Running validation...\n",
            "Validation cross-entropy: 4.674\n",
            "Batch 31\n",
            "Batch 32\n",
            "Batch 33\n",
            "Batch 34\n",
            "Batch 35\n",
            "Batch 36\n",
            "Batch 37\n",
            "Batch 38\n",
            "Batch 39\n",
            "Batch 40\n",
            "Batch 40 Train CE 4.652\n",
            "Running validation...\n",
            "Validation cross-entropy: 4.640\n",
            "Batch 41\n",
            "Batch 42\n",
            "Batch 43\n",
            "Batch 44\n",
            "Batch 45\n",
            "Batch 46\n",
            "Batch 47\n",
            "Batch 48\n",
            "Batch 49\n",
            "Batch 50\n",
            "Batch 50 Train CE 4.627\n",
            "Running validation...\n",
            "Validation cross-entropy: 4.611\n",
            "Batch 51\n",
            "Batch 52\n",
            "Batch 53\n",
            "Batch 54\n",
            "Batch 55\n",
            "Batch 56\n",
            "Batch 57\n",
            "Batch 58\n",
            "Batch 59\n",
            "Batch 60\n",
            "Batch 60 Train CE 4.601\n",
            "Running validation...\n",
            "Validation cross-entropy: 4.597\n",
            "Batch 61\n",
            "Batch 62\n",
            "Batch 63\n",
            "Batch 64\n",
            "Batch 65\n",
            "Batch 66\n",
            "Batch 67\n",
            "Batch 68\n",
            "Batch 69\n",
            "Batch 70\n",
            "Batch 70 Train CE 4.594\n",
            "Running validation...\n",
            "Validation cross-entropy: 4.591\n",
            "Batch 71\n",
            "Batch 72\n",
            "Batch 73\n",
            "Batch 74\n",
            "Batch 75\n",
            "Batch 76\n",
            "Batch 77\n",
            "Batch 78\n",
            "Batch 79\n",
            "Batch 80\n",
            "Batch 80 Train CE 4.584\n",
            "Running validation...\n",
            "Validation cross-entropy: 4.595\n",
            "Validation error increasing!  Training stopped.\n",
            "\n",
            "Duration: 0:05:58.055595\n",
            "Final training cross-entropy: 4.583\n",
            "Final validation cross-entropy: 4.589\n",
            "Final test cross-entropy: 4.594\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10-final"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}